{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O0mwbfcus9ia"
   },
   "source": [
    "This is a companion notebook for the book [Deep Learning with Python, Second Edition](https://www.manning.com/books/deep-learning-with-python-second-edition?a_aid=keras&a_bid=76564dff). For readability, it only contains runnable code blocks and section titles, and omits everything else in the book: text paragraphs, figures, and pseudocode.\n",
    "\n",
    "**If you want to be able to follow what's going on, I recommend reading the notebook side by side with your copy of the book.**\n",
    "\n",
    "This notebook was generated for TensorFlow 2.6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "73KFuKHVs9ic"
   },
   "source": [
    "## Beyond text classification: Sequence-to-sequence learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dxlaNoLqs9id"
   },
   "source": [
    "### A machine translation example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t919swd_s9id",
    "outputId": "ff6c3d94-c4a7-40d8-8c76-a1c380916b94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-10-14 05:42:10--  http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.130.207, 74.125.68.207, 64.233.170.207, ...\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.130.207|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2638744 (2.5M) [application/zip]\n",
      "Saving to: ‘spa-eng.zip’\n",
      "\n",
      "spa-eng.zip         100%[===================>]   2.52M  2.06MB/s    in 1.2s    \n",
      "\n",
      "2023-10-14 05:42:11 (2.06 MB/s) - ‘spa-eng.zip’ saved [2638744/2638744]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
    "!unzip -q spa-eng.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add start and end token for all the output sentences. This is needed for model to know the begining and ending of a sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "LIZsND79s9ie"
   },
   "outputs": [],
   "source": [
    "text_file = \"spa-eng/spa.txt\"\n",
    "with open(text_file) as f:\n",
    "    lines = f.read().split(\"\\n\")[:-1]\n",
    "text_pairs = []\n",
    "for line in lines:\n",
    "    english, spanish = line.split(\"\\t\")\n",
    "    spanish = \"[start] \" + spanish + \" [end]\"\n",
    "    text_pairs.append((english, spanish))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print a random sentence pair (English and its corresponding spanish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EB5e3198s9if",
    "outputId": "8365f805-23c6-4cae-ec6b-b28291a9b15b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('I told him nothing.', '[start] No le dije nada. [end]')\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "print(random.choice(text_pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code shuffles a list of text pairs and splits it into training, validation, and test sets.\n",
    "\n",
    "The `text_pairs` variable is a list of tuples, where each tuple contains an English sentence and its corresponding Spanish translation.\n",
    "\n",
    "The `random.shuffle` function shuffles the list of text pairs randomly, which is a common practice to ensure that the data is not biased in any way.\n",
    "\n",
    "The `num_val_samples` variable is set to 15% of the total number of text pairs, which determines the size of the validation set.\n",
    "\n",
    "The `num_train_samples` variable is set to the remaining number of text pairs after subtracting twice the size of the validation set, which determines the size of the training set.\n",
    "\n",
    "The `train_pairs` variable is a slice of the `text_pairs` list that contains the first `num_train_samples` text pairs, which are used for training the model.\n",
    "\n",
    "The `val_pairs` variable is a slice of the `text_pairs` list that contains the next `num_val_samples` text pairs, which are used for validating the model during training.\n",
    "\n",
    "The `test_pairs` variable is a slice of the `text_pairs` list that contains the remaining text pairs, which are used for testing the model after training.\n",
    "\n",
    "By splitting the data into training, validation, and test sets, we can evaluate the performance of the model on unseen data and prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "36No6dbFs9ig"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(text_pairs)\n",
    "num_val_samples = int(0.15 * len(text_pairs))\n",
    "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
    "train_pairs = text_pairs[:num_train_samples]\n",
    "val_pairs = text_pairs[num_train_samples:num_train_samples + num_val_samples]\n",
    "test_pairs = text_pairs[num_train_samples + num_val_samples:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nq_z1aBgs9ih"
   },
   "source": [
    "## Vectorizing the English and Spanish text pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code defines a function `custom_standardization` that takes an input string and applies two text preprocessing steps to it:\n",
    "\n",
    "1. Convert the input string to lowercase using the `tf.strings.lower` function.\n",
    "2. Remove any characters in the `strip_chars` string from the input string using the `tf.strings.regex_replace` function.\n",
    "\n",
    "The `strip_chars` string is defined outside of this function and contains a set of characters that should be removed from the input string. The `re.escape` function is used to escape any special characters in the `strip_chars` string so that they can be used in a regular expression pattern.\n",
    "\n",
    "The `tf.strings.regex_replace` function takes three arguments: the input string, the regular expression pattern to match, and the replacement string. In this case, the regular expression pattern is set to `f\"[{re.escape(strip_chars)}]\"`, which matches any character in the `strip_chars` string. The replacement string is an empty string, which effectively removes any matched characters from the input string.\n",
    "\n",
    "Overall, this function can be used as a text preprocessing step to standardize the input text by converting it to lowercase and removing any unwanted characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import string\n",
    "import re\n",
    "\n",
    "strip_chars = string.punctuation + \"¿\"\n",
    "strip_chars = strip_chars.replace(\"[\", \"\")\n",
    "strip_chars = strip_chars.replace(\"]\", \"\")\n",
    "\n",
    "def custom_standardization(input_string):\n",
    "    lowercase = tf.strings.lower(input_string)\n",
    "    return tf.strings.regex_replace(\n",
    "        lowercase, f\"[{re.escape(strip_chars)}]\", \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code sets up text vectorization layers for the source and target languages in a machine translation model. \n",
    "\n",
    "The `source_vectorization` layer is used to convert English text into integer sequences. It is initialized with a `max_tokens` parameter of 15000, which sets the maximum size of the vocabulary. The `output_mode` parameter is set to \"int\", which means that the layer will output integer sequences. The `output_sequence_length` parameter is set to 20, which means that the layer will pad or truncate sequences to a length of 20 tokens. \n",
    "\n",
    "The `target_vectorization` layer is used to convert Spanish text into integer sequences. It is initialized with the same `max_tokens` and `output_sequence_length` parameters as the `source_vectorization` layer, but with an additional token for the end-of-sequence marker. The `standardize` parameter is set to `custom_standardization`, which is a function that preprocesses the text by converting it to lowercase and removing unwanted characters.\n",
    "\n",
    "The `train_english_texts` and `train_spanish_texts` variables are lists of English and Spanish sentences, respectively, extracted from the `train_pairs` dataset. These lists are used to adapt the vectorization layers to the training data using the `adapt` method.\n",
    "\n",
    "Overall, this code sets up the text preprocessing layers for the machine translation model, which will convert the input text into integer sequences that can be fed into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Qe8l18zes9ii"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "vocab_size = 15000\n",
    "sequence_length = 20\n",
    "\n",
    "source_vectorization = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length,\n",
    ")\n",
    "target_vectorization = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length + 1,\n",
    "    standardize=custom_standardization,\n",
    ")\n",
    "train_english_texts = [pair[0] for pair in train_pairs]\n",
    "train_spanish_texts = [pair[1] for pair in train_pairs]\n",
    "source_vectorization.adapt(train_english_texts)\n",
    "target_vectorization.adapt(train_spanish_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SNz9CXqjs9ij"
   },
   "source": [
    "## Preparing datasets for the translation task "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code defines functions to format and create TensorFlow datasets for a machine translation model. \n",
    "\n",
    "The `format_dataset` function takes two arguments, `eng` and `spa`, which are English and Spanish sentences, respectively. It applies the `source_vectorization` and `target_vectorization` layers to the input sentences to convert them into integer sequences. It then returns a dictionary with two keys, \"english\" and \"spanish\", which map to the English and Spanish integer sequences, respectively. The \"spanish\" sequence is shifted by one position to the right, so that it can be used as the target sequence for the model.\n",
    "\n",
    "The `make_dataset` function takes a list of sentence pairs as input. It extracts the English and Spanish sentences from the pairs and creates a TensorFlow dataset using the `tf.data.Dataset.from_tensor_slices` method. It then applies the `format_dataset` function to each element of the dataset using the `map` method. The dataset is batched using the `batch` method with a batch size of 64. Finally, the dataset is shuffled, prefetched, and cached using the `shuffle`, `prefetch`, and `cache` methods, respectively.\n",
    "\n",
    "The `train_ds` and `val_ds` variables are created by calling the `make_dataset` function on the training and validation pairs, respectively. These variables represent the formatted and batched datasets that can be used to train and evaluate the machine translation model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "jbf_16y_s9ij"
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "def format_dataset(eng, spa):\n",
    "    eng = source_vectorization(eng)\n",
    "    spa = target_vectorization(spa)\n",
    "    return ({\n",
    "        \"english\": eng,\n",
    "        \"spanish\": spa[:, :-1],\n",
    "    }, spa[:, 1:])\n",
    "\n",
    "def make_dataset(pairs):\n",
    "    eng_texts, spa_texts = zip(*pairs)\n",
    "    eng_texts = list(eng_texts)\n",
    "    spa_texts = list(spa_texts)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, spa_texts))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(format_dataset, num_parallel_calls=4)\n",
    "    return dataset.shuffle(2048).prefetch(16).cache()\n",
    "\n",
    "train_ds = make_dataset(train_pairs)\n",
    "val_ds = make_dataset(val_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is used to print the shapes of the input and target data for the machine translation model. \n",
    "\n",
    "The `train_ds` dataset is created using the `make_dataset` function and contains batches of English and Spanish sentences that have been converted into integer sequences. The `take(1)` method is used to extract the first batch from the dataset.\n",
    "\n",
    "The `for` loop iterates over the first batch of data and assigns the English and Spanish integer sequences to the `inputs` dictionary and the shifted Spanish integer sequences to the `targets` variable.\n",
    "\n",
    "The `print` statements then print the shapes of the `inputs['english']`, `inputs['spanish']`, and `targets` arrays. The `inputs['english']` array has a shape of `(batch_size, sequence_length)`, where `batch_size` is the number of sentences in the batch and `sequence_length` is the length of each sentence after padding or truncation. The `inputs['spanish']` array has the same shape as `inputs['english']`. The `targets` array has a shape of `(batch_size, sequence_length)`, where `sequence_length` is one less than the length of the input sequences, since the target sequence is shifted by one position to the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9PQpPFdDs9ik",
    "outputId": "1dcc1b65-0a51-41bd-c25e-6d40be426edf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs['english'].shape: (64, 20)\n",
      "inputs['spanish'].shape: (64, 20)\n",
      "targets.shape: (64, 20)\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in train_ds.take(1):\n",
    "    print(f\"inputs['english'].shape: {inputs['english'].shape}\")\n",
    "    print(f\"inputs['spanish'].shape: {inputs['spanish'].shape}\")\n",
    "    print(f\"targets.shape: {targets.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZgMOtKwos9ik"
   },
   "source": [
    "# Sequence-to-sequence learning with RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2BmRfENks9ik"
   },
   "source": [
    "## GRU-based encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code defines the encoder part of a sequence-to-sequence model for machine translation using Keras.\n",
    "\n",
    "The `source` variable is an input tensor that represents the English sentence. It is defined using the `Input` function from the Keras `layers` module, with a shape of `(None,)` to allow for variable-length sequences.\n",
    "\n",
    "The `Embedding` layer maps the input integers to dense vectors of fixed size. It is initialized with a `vocab_size` parameter that sets the size of the vocabulary, an `embed_dim` parameter that sets the dimensionality of the embedding space, and a `mask_zero` parameter that masks the zero-padding in the input sequence.\n",
    "\n",
    "The `Bidirectional` layer applies a GRU layer to the input sequence in both forward and backward directions, and merges the outputs using the \"sum\" method. This allows the encoder to take into account both past and future context when generating the encoded representation of the input sequence.\n",
    "\n",
    "The `encoded_source` variable represents the encoded representation of the input sequence, which is a fixed-size vector that summarizes the input sequence. This vector can be used as the initial hidden state of the decoder part of the sequence-to-sequence model.\n",
    "\n",
    "Note that the `name` parameter is used to give a name to the input tensor, which can be useful for debugging and visualization purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "uAROQqles9ik"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "embed_dim = 256\n",
    "latent_dim = 1024\n",
    "\n",
    "source = keras.Input(shape=(None,), dtype=\"int64\", name=\"english\")\n",
    "x = layers.Embedding(vocab_size, embed_dim, mask_zero=True)(source)\n",
    "encoded_source = layers.Bidirectional(\n",
    "    layers.GRU(latent_dim), merge_mode=\"sum\")(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iuVOpa2as9il"
   },
   "source": [
    "## GRU-based decoder and the end-to-end model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code defines the decoder part of a sequence-to-sequence model for machine translation using Keras.\n",
    "\n",
    "The `past_target` variable is an input tensor that represents the target sequence up to the current time step. It is defined using the `Input` function from the Keras `layers` module, with a shape of `(None,)` to allow for variable-length sequences.\n",
    "\n",
    "The `Embedding` layer maps the input integers to dense vectors of fixed size. It is initialized with a `vocab_size` parameter that sets the size of the vocabulary, an `embed_dim` parameter that sets the dimensionality of the embedding space, and a `mask_zero` parameter that masks the zero-padding in the input sequence.\n",
    "\n",
    "The `GRU` layer is a type of recurrent neural network layer that applies a gated recurrent unit to the input sequence. It is initialized with a `latent_dim` parameter that sets the dimensionality of the output space, and a `return_sequences` parameter that specifies whether to return the full sequence of outputs or only the last output.\n",
    "\n",
    "The `initial_state` parameter of the `decoder_gru` layer is set to `encoded_source`, which is the output of the encoder part of the model. This initializes the hidden state of the decoder GRU with the final hidden state of the encoder GRU, which allows the decoder to take into account the entire input sequence when generating the output sequence.\n",
    "\n",
    "The `Dropout` layer randomly sets a fraction of the input units to 0 at each update during training time, which helps prevent overfitting.\n",
    "\n",
    "The `Dense` layer applies a softmax activation function to the output of the previous layer, which produces a probability distribution over the vocabulary for each position in the target sequence.\n",
    "\n",
    "Finally, the `seq2seq_rnn` model is defined using the `Model` function from the Keras `models` module, with `source` and `past_target` as inputs and `target_next_step` as output. This model can be used to train and evaluate the sequence-to-sequence model for machine translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "slmu0Gc-s9il"
   },
   "outputs": [],
   "source": [
    "past_target = keras.Input(shape=(None,), dtype=\"int64\", name=\"spanish\")\n",
    "x = layers.Embedding(vocab_size, embed_dim, mask_zero=True)(past_target)\n",
    "decoder_gru = layers.GRU(latent_dim, return_sequences=True)\n",
    "x = decoder_gru(x, initial_state=encoded_source)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "target_next_step = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
    "seq2seq_rnn = keras.Model([source, past_target], target_next_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tSa8dczus9il"
   },
   "source": [
    "## Training our recurrent sequence-to-sequence model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code compiles and trains the sequence-to-sequence model for machine translation using Keras.\n",
    "\n",
    "The `compile` method configures the model for training by specifying the optimizer, loss function, and evaluation metrics. In this case, the optimizer is set to \"rmsprop\", which is a popular optimization algorithm for deep learning. The loss function is set to \"sparse_categorical_crossentropy\", which is a common choice for multi-class classification problems with integer labels. The evaluation metric is set to \"accuracy\", which measures the proportion of correctly classified examples.\n",
    "\n",
    "The `fit` method trains the model on the training dataset (`train_ds`) for a specified number of epochs (15 in this case), and validates the model on the validation dataset (`val_ds`) after each epoch. During training, the model updates its parameters to minimize the loss function using backpropagation and the optimizer. The validation dataset is used to monitor the performance of the model on unseen data and prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5fq8lOE5s9il",
    "outputId": "737cc1d8-f4d0-43d5-ca8e-f71a654fa5f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1302/1302 [==============================] - 150s 101ms/step - loss: 4.6762 - accuracy: 0.3188 - val_loss: 3.9273 - val_accuracy: 0.3865\n",
      "Epoch 2/15\n",
      "1302/1302 [==============================] - 111s 85ms/step - loss: 3.7148 - accuracy: 0.4162 - val_loss: 3.2840 - val_accuracy: 0.4660\n",
      "Epoch 3/15\n",
      "1302/1302 [==============================] - 110s 85ms/step - loss: 3.2060 - accuracy: 0.4733 - val_loss: 2.8816 - val_accuracy: 0.5171\n",
      "Epoch 4/15\n",
      "1302/1302 [==============================] - 110s 85ms/step - loss: 2.8603 - accuracy: 0.5136 - val_loss: 2.6377 - val_accuracy: 0.5507\n",
      "Epoch 5/15\n",
      "1302/1302 [==============================] - 120s 93ms/step - loss: 2.5877 - accuracy: 0.5464 - val_loss: 2.4625 - val_accuracy: 0.5746\n",
      "Epoch 6/15\n",
      "1302/1302 [==============================] - 111s 86ms/step - loss: 2.3629 - accuracy: 0.5760 - val_loss: 2.3263 - val_accuracy: 0.5956\n",
      "Epoch 7/15\n",
      "1302/1302 [==============================] - 111s 86ms/step - loss: 2.1787 - accuracy: 0.5990 - val_loss: 2.2306 - val_accuracy: 0.6103\n",
      "Epoch 8/15\n",
      "1302/1302 [==============================] - 111s 85ms/step - loss: 2.0191 - accuracy: 0.6202 - val_loss: 2.1597 - val_accuracy: 0.6214\n",
      "Epoch 9/15\n",
      "1302/1302 [==============================] - 111s 86ms/step - loss: 1.8846 - accuracy: 0.6385 - val_loss: 2.1090 - val_accuracy: 0.6289\n",
      "Epoch 10/15\n",
      "1302/1302 [==============================] - 110s 85ms/step - loss: 1.7621 - accuracy: 0.6553 - val_loss: 2.0594 - val_accuracy: 0.6376\n",
      "Epoch 11/15\n",
      "1302/1302 [==============================] - 120s 92ms/step - loss: 1.6577 - accuracy: 0.6696 - val_loss: 2.0305 - val_accuracy: 0.6436\n",
      "Epoch 12/15\n",
      "1302/1302 [==============================] - 110s 85ms/step - loss: 1.5662 - accuracy: 0.6823 - val_loss: 1.9972 - val_accuracy: 0.6485\n",
      "Epoch 13/15\n",
      "1302/1302 [==============================] - 110s 85ms/step - loss: 1.4855 - accuracy: 0.6943 - val_loss: 1.9720 - val_accuracy: 0.6529\n",
      "Epoch 14/15\n",
      "1302/1302 [==============================] - 110s 85ms/step - loss: 1.4125 - accuracy: 0.7053 - val_loss: 1.9556 - val_accuracy: 0.6552\n",
      "Epoch 15/15\n",
      "1302/1302 [==============================] - 110s 84ms/step - loss: 1.3501 - accuracy: 0.7145 - val_loss: 1.9542 - val_accuracy: 0.6570\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7ff1c07b9f00>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq2seq_rnn.compile(\n",
    "    optimizer=\"rmsprop\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"])\n",
    "seq2seq_rnn.fit(train_ds, epochs=15, validation_data=val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s6G_TVc3s9il"
   },
   "source": [
    "# Translating new sentences with our RNN encoder and decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code defines a function `decode_sequence` that uses the trained sequence-to-sequence model to translate an input sentence from English to Spanish.\n",
    "\n",
    "The `spa_vocab` variable is a list of words in the Spanish vocabulary, obtained from the target vectorization layer.\n",
    "\n",
    "The `spa_index_lookup` variable is a dictionary that maps integer indices to words in the Spanish vocabulary.\n",
    "\n",
    "The `max_decoded_sentence_length` variable sets the maximum length of the output sentence.\n",
    "\n",
    "The `decode_sequence` function takes an input sentence in English as a string, tokenizes it using the source vectorization layer, and initializes the decoded sentence as the string \"[start]\".\n",
    "\n",
    "The function then enters a loop that iteratively predicts the next word in the output sentence using the trained sequence-to-sequence model. At each iteration, the function tokenizes the current decoded sentence using the target vectorization layer, and passes both the tokenized input sentence and the tokenized decoded sentence to the model's `predict` method. The `predict` method returns a tensor of shape `(batch_size, sequence_length, vocab_size)` that contains the predicted probabilities for each word in the Spanish vocabulary at each position in the output sequence.\n",
    "\n",
    "The function then selects the word with the highest predicted probability at the current position, using the `argmax` method of the NumPy library. The selected word is then appended to the decoded sentence, and the loop continues until either the maximum length is reached or the special token \"[end]\" is predicted.\n",
    "\n",
    "Finally, the function returns the decoded sentence as a string.\n",
    "\n",
    "The code then defines a list `test_eng_texts` that contains the English sentences from the test set, and uses the `decode_sequence` function to translate 20 randomly selected sentences from the test set. The input sentence and the corresponding translated sentence are printed for each example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gAMlmMSJs9il",
    "outputId": "481c25fb-82a7-4fa2-a34a-4d92e5500be6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "I'm a councillor, too.\n",
      "1/1 [==============================] - 4s 4s/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "[start] soy un [UNK] [end]\n",
      "-\n",
      "We're going to Boston.\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "[start] vamos a boston [end]\n",
      "-\n",
      "The boy has an apple in his pocket.\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "[start] el niño tiene una manzana en su bolsillo [end]\n",
      "-\n",
      "Tom has been crying all night.\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "[start] tom ha estado viendo la noche [end]\n",
      "-\n",
      "Tom told me that he didn't want to come home empty-handed.\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "[start] tom me dijo que no quería venir a casa [end]\n",
      "-\n",
      "Republicans were furious.\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "[start] [UNK] [UNK] [end]\n",
      "-\n",
      "May I borrow your dictionary?\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "[start] me puedes prestar tu diccionario [end]\n",
      "-\n",
      "I visited my grandmother's house.\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "[start] visité la casa de mi casa [end]\n",
      "-\n",
      "He can't have said such a stupid thing.\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "[start] no puede haber dicho tal estupidez [end]\n",
      "-\n",
      "I wish there were more people like you.\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "[start] ojalá más que fuera más gente [end]\n",
      "-\n",
      "I had a run of bad luck.\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "[start] tuve una mala [UNK] [end]\n",
      "-\n",
      "You don't need to thank me. I'm here to serve you.\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "[start] no necesitas que [UNK] aquí para mí [end]\n",
      "-\n",
      "She just told me.\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "[start] ella me dijo que me [UNK] [end]\n",
      "-\n",
      "We have two dogs, three cats, and six chickens.\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "[start] tenemos dos perros y tres autos [end]\n",
      "-\n",
      "Why don't you tell me about it?\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "[start] por qué no me lo [UNK] [end]\n",
      "-\n",
      "They looked at the photo taken of me when I was a boy and laughed.\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "[start] ellos se [UNK] en la pared cuando se me ha llamado un hombre y [UNK] [end]\n",
      "-\n",
      "How much money do you want?\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "[start] cuánto dinero quieres [end]\n",
      "-\n",
      "Just give me a moment.\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "[start] solo dame un momento [end]\n",
      "-\n",
      "She calculates faster than any other student.\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "[start] ella se [UNK] más rápido que japón [end]\n",
      "-\n",
      "Thank you very much for driving me all the way to my house.\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "[start] gracias por llevarme a mi amigo para que mi casa [end]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "spa_vocab = target_vectorization.get_vocabulary()\n",
    "spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n",
    "max_decoded_sentence_length = 20\n",
    "\n",
    "def decode_sequence(input_sentence):\n",
    "    tokenized_input_sentence = source_vectorization([input_sentence])\n",
    "    decoded_sentence = \"[start]\"\n",
    "    for i in range(max_decoded_sentence_length):\n",
    "        tokenized_target_sentence = target_vectorization([decoded_sentence])\n",
    "        next_token_predictions = seq2seq_rnn.predict(\n",
    "            [tokenized_input_sentence, tokenized_target_sentence])\n",
    "        sampled_token_index = np.argmax(next_token_predictions[0, i, :])\n",
    "        sampled_token = spa_index_lookup[sampled_token_index]\n",
    "        decoded_sentence += \" \" + sampled_token\n",
    "        if sampled_token == \"[end]\":\n",
    "            break\n",
    "    return decoded_sentence\n",
    "\n",
    "test_eng_texts = [pair[0] for pair in test_pairs]\n",
    "for _ in range(20):\n",
    "    input_sentence = random.choice(test_eng_texts)\n",
    "    print(\"-\")\n",
    "    print(input_sentence)\n",
    "    print(decode_sequence(input_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ajQEJlRrs9in"
   },
   "source": [
    "## Summary"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
